---
title: Home
layout: home
---

# Robot Learning

| Date | Lecture        | Lab          | Lecturer | Homework |
|:--|:--|:--|:--|:--|
| 23.9.2024 | Lec 1: Machine learning 101: model, loss, learning, issues, regression, classification | Intro: ML | Ales | - |
| 30.9.2024 | Lec 2: Under the hood of a linear classifier: two-class and multi-class linear classifier on RGB images  | 1D regression and 2D classification: Revision of the regression and classification theory, analytic gradient computation, gradient in computational graph and loss minimization. | Karel  | - |
| 7.10.2024 | Lec 3: Where the hell does the loss come from? MAP and ML estimate, KL divergence and losses. | Loss, MLP  | Ales | HW1 - MLP |
| 14.10.2024 | Lec 4: The story of the cat's brain surgery: fully-connected NN + fast backpropagation via Vector-Jacobian-Product (VJP), cortex + convolutional layer  | Convolutional neural networks. | Honza | - |
| 21.10.2024 | Lec 5: Under the hood of auto-differentiation: Vector-Jacobian-Product (VJP) vs chainrule and multiplication of Jacobians, convolutional layer and its VJP | Backpropagation | Honza | HW2 - Autograd |
| 28.10.2024 | Den vzniku samostatného československa | - | -  | - |
| 4.11.2024 | **Midterm test** | - | -  | - |
| 11.11.2024 | Lec 6: Why is learning prone to fail? - Structural issues: layers + issues, batch-norm, drop-out | Layers | Roman | HW3 - Segmentation |
| 18.11.2024 | Lec 7: Why is learning prone to fail? - Optimization issues: optimization vs learning, KL divergence, SGD, momentum, convergence rate, Adagrad, RMSProp, AdamOptimizer, diminishing/exploding gradient, oscillation, double descent  | Optimization | Karel | - |
| 25.11.2024 | Lec 8: Reinforcement learning: Approximated Q-learning, DQN, DDPG, Derivation of the policy gradient (REINFORCE), A2C, TRPO, PPO, Reward shaping, Inverse RL, Applications,  | Reinforcement learning I | David | - |
| 2.12.2024 | Lec 9: What I wanted to speak about from the very beginning, but it did not fit in: Implicit layers (Backpropagation through unconstrained and constrained optimization problems, ODE solvers, roots, fixed points) + existing end-to-end differentiable modules cvxpy, gradSLAM, gradMPC, gradODE, pytorch3d, Memory and attention (recurrent nets, Image transformers with attention module)  | Reinforcement learning II | David | HW4 - RL |
| 9.12.2024 | Lec 10: What can('t) we do with a deep net?: Classification (ResNet, Squeeze and Excitation Nets), Segmentation (DeepLab), Detection (Yolo, fast-RCNN), Regression (OpenPose), Spatial Transformer Nets | Architectures | Roman | - |
| 16.12.2024 | Lec 11: | - | -  | - |
| 6.1.2024 | **Exam** | - | -  | - |



